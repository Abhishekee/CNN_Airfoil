{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fddd3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import torch\n",
    "import torchvision\n",
    "import csv\n",
    "import pandas as pd\n",
    "import imageio\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os, sys\n",
    "import glob\n",
    "import json\n",
    "import random\n",
    "import ipdb\n",
    "import base64\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "import matplotlib.image as mpimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1ca377d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(2,20, kernel_size=(5,5), stride=(1,1))\n",
    "        nn.init.uniform_(self.conv1.weight)\n",
    "        self.bn1 = nn.BatchNorm2d(20)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(20,10, kernel_size=(5,5), stride=(1,1))\n",
    "        nn.init.uniform_(self.conv2.weight)\n",
    "        self.bn1 = nn.BatchNorm2d(10)\n",
    "        \n",
    "        self.fc1 = nn.Linear(1690,500)\n",
    "        nn.init.uniform_(self.fc1.weight)\n",
    "        self.bn3 = nn.BatchNorm1d(500)\n",
    "        \n",
    "        self.fc2 = nn.Linear(500,100)\n",
    "        nn.init.uniform_(self.fc2.weight)\n",
    "        self.bn4 = nn.BatchNorm1d(100)\n",
    "        \n",
    "        self.fc3 = nn.Linear(100,1)\n",
    "        nn.init.uniform_(self.fc3.weight)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.max_pool2d(x, kernel_size = 2, stride=2, padding = 0,dilation=1,ceil_mode=False)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = F.max_pool2d(x, kernel_size = 2, stride=2, padding = 0,dilation=1,ceil_mode=False)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = x.view(x.size(0), 1690)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = F.dropout(x, p=0.5)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = F.dropout(x, p=0.5)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        x = F.sigmoid(x)\n",
    "        \n",
    "        \n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "def659b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def importdata(data, batch_size = 50):\n",
    "    np.random.shuffle(data)\n",
    "    batches = []\n",
    "    \n",
    "    re = data[:,1].astype(np.float64)\n",
    "    re -= re.mean(0)\n",
    "    re /= re.var(0)\n",
    "    re = re.reshape(re.shape[0],1)\n",
    "    re = re.astype(np.str)\n",
    "    \n",
    "    data = np.delete(data,1,1)\n",
    "    data = np.insert(data,[1],re,axis=1)\n",
    "    \n",
    "    train_set = data[:15000,:]\n",
    "    test_set = data[15000:,:]\n",
    "    \n",
    "    batch_number_train = int(train_set.shape[0]/batch_size)\n",
    "    training_data = []\n",
    "    \n",
    "    for idx in range (0, batch_number_train):\n",
    "        data = train_set[idx*batch_size:(idx + 1)*batch_size]\n",
    "        training_data.append(data)\n",
    "        \n",
    "    training_data = np.array(training_data)\n",
    "    \n",
    "    batch_number_test = int(test_set.shape[0]/batch_size)\n",
    "    testing_data = []\n",
    "    \n",
    "    for idx in range(0, batch_number_test):\n",
    "        data = test_set[idx*batch_size:(idx+1)*batch_size,:]\n",
    "        testing_data.append(data)\n",
    "        \n",
    "    testing_data = np.array(testing_data)\n",
    "    \n",
    "    return training_data, testing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45d23629",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(importtrain, optimizer, criterion, epoch, net, scope = 'cl'):\n",
    "    net.train()\n",
    "    train_loss_sum = 0\n",
    "    for idx, batch in enumerate(importtrain):\n",
    "        image_batch = []\n",
    "        \n",
    "        if scope == 'cl':\n",
    "            target = (importtrain[idx,:,3]).reshape((len(importtrain[idx,:,3]),1))\n",
    "            target = target.astype(float)\n",
    "            target = torch.from_numpy(target).float()\n",
    "            \n",
    "        if scope == 'cd':\n",
    "            target = (importtrain[idx,:,4]).reshape((len(importtrain[idx,:,3]),1))\n",
    "            target = target.astype(float)\n",
    "            target = torch.from_numpy(target).float()\n",
    "            \n",
    "        if scope == 'ld_ratio':\n",
    "            target = (importtrain[idx,:,5]).reshape((len(importtrain[idx,:,3]),1))\n",
    "            target = target.astype(float)\n",
    "            target = torch.from_numpy(target).float()\n",
    "            \n",
    "        for k,row in enumerate(batch):\n",
    "            img_name = importtrain[idx,k,0]+'_'+str(int(float(importtrain[idx,k,2])))+'.png'\n",
    "            img = imageio.imread('./naca_imgs/'+img_name)\n",
    "            re_array = np.full((64,64),importtrain[idx,k,1])\n",
    "            img = np.dstack((re_array,img))\n",
    "            image_batch.append(img)\n",
    "            \n",
    "        image_batch=np.array(image_batch,dtype = float)\n",
    "        image_batch = np.transpose(image_batch,(0,3,1,2))\n",
    "            \n",
    "        image_batch = torch.from_numpy(image_batch).float()\n",
    "        output = net(image_batch)\n",
    "        loss = criterion(output,target)\n",
    "            \n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_sum += float(loss)\n",
    "        if idx % 50 == 0:\n",
    "            print('epoch ', epoch, 'no_batch ', idx, 'Train loss ', loss)\n",
    "                \n",
    "    train_loss = train_loss_sum/importtrain.shape[0]\n",
    "    print(\"epoch \",epoch, \"epoch_loss \", train_loss)\n",
    "    return train_loss\n",
    "    \n",
    "    \n",
    "def test(importtest, criterion,epoch,epochs,net,scope = 'cl'):\n",
    "        net.eval()\n",
    "        test_loss_sum = 0\n",
    "        final_epoch = []\n",
    "        \n",
    "        for idx,batch in enumerate(importtest):\n",
    "            image_batch =[]\n",
    "            if epoch == epochs - 1:\n",
    "                final_batch = batch\n",
    "                \n",
    "            if scope == 'cl':\n",
    "                target = importtest[idx, :,3].reshape((len(importtest[idx,:,3]),1))\n",
    "                target = target.astype(float)\n",
    "                target = torch.from_numpy(target).float()\n",
    "                \n",
    "                \n",
    "            if scope == 'cd':\n",
    "                target = importtest[idx, :,4].reshape((len(importtest[idx,:,3]),1))\n",
    "                target = target.astype(float)\n",
    "                target = torch.from_numpy(target).float()\n",
    "                \n",
    "                \n",
    "            if scope == 'ld_ratio':\n",
    "                target = importtest[idx, :,5].reshape((len(importtest[idx,:,3]),1))\n",
    "                target = target.astype(float)\n",
    "                target = torch.from_numpy(target).float()\n",
    "                \n",
    "                \n",
    "            for k,row in enumerate(batch):\n",
    "                img_name = importtest[idx,k,0]+'_'+str(int(float(importtest[idx,k,2])))+'.png'\n",
    "                img = imageio.imread('./naca_imgs/'+img_name)\n",
    "                re_array = np.full((64,64),importtest[idx,k,1])\n",
    "                img = np.dstack((re_array,img))\n",
    "                image_batch.append(img)\n",
    "                \n",
    "                \n",
    "            image_batch = np.array(image_batch,dtype = float)\n",
    "            image_batch = np.transpose(image_batch,(0,3,1,2))\n",
    "            image_batch = torch.from_numpy(image_batch).float()\n",
    "            \n",
    "            \n",
    "            output = net(image_batch)\n",
    "            \n",
    "            loss = criterion(output,target)\n",
    "            \n",
    "            test_loss_sum += float(loss)\n",
    "            \n",
    "            if epoch == epochs-1:\n",
    "                output = output.detach().numpy()\n",
    "                final_batch = np.insert(final_batch,[5], output,axis = 1)\n",
    "                final_epoch.append(final_batch)\n",
    "                \n",
    "            else:\n",
    "                final_epoch = 5\n",
    "                \n",
    "            if idx%20 == 0:\n",
    "                print('epoch ', epoch, 'no_batch ', idx, 'Test loss ', loss)\n",
    "                \n",
    "            test_loss = test_loss_sum/importtest.shape[0]\n",
    "            print(\"epoch \",epoch, \"epoch_loss \", test_loss)\n",
    "            return test_loss, final_epoch\n",
    "            \n",
    "        \n",
    "        \n",
    "            \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "514e82a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aplus\\AppData\\Local\\Temp\\ipykernel_28680\\3275313959.py:9: DeprecationWarning: `np.str` is a deprecated alias for the builtin `str`. To silence this warning, use `str` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.str_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  re = re.astype(np.str)\n",
      "D:\\Anaconda\\lib\\site-packages\\torch\\nn\\functional.py:1944: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  0 no_batch  0 Train loss  tensor(1.6959, grad_fn=<MseLossBackward0>)\n",
      "epoch  0 no_batch  50 Train loss  tensor(1.6408, grad_fn=<MseLossBackward0>)\n",
      "epoch  0 no_batch  100 Train loss  tensor(2.1361, grad_fn=<MseLossBackward0>)\n",
      "epoch  0 no_batch  150 Train loss  tensor(1.5446, grad_fn=<MseLossBackward0>)\n",
      "epoch  0 no_batch  200 Train loss  tensor(1.6300, grad_fn=<MseLossBackward0>)\n",
      "epoch  0 no_batch  250 Train loss  tensor(1.3798, grad_fn=<MseLossBackward0>)\n",
      "epoch  0 epoch_loss  1.582734574874242\n",
      "epoch  0 no_batch  0 Test loss  tensor(1.5342, grad_fn=<MseLossBackward0>)\n",
      "epoch  0 epoch_loss  0.017046484682295058\n",
      "epoch  1 no_batch  0 Train loss  tensor(1.6959, grad_fn=<MseLossBackward0>)\n",
      "epoch  1 no_batch  50 Train loss  tensor(1.6408, grad_fn=<MseLossBackward0>)\n",
      "epoch  1 no_batch  100 Train loss  tensor(2.1361, grad_fn=<MseLossBackward0>)\n",
      "epoch  1 no_batch  150 Train loss  tensor(1.5446, grad_fn=<MseLossBackward0>)\n",
      "epoch  1 no_batch  200 Train loss  tensor(1.6300, grad_fn=<MseLossBackward0>)\n",
      "epoch  1 no_batch  250 Train loss  tensor(1.3798, grad_fn=<MseLossBackward0>)\n",
      "epoch  1 epoch_loss  1.582734574874242\n",
      "epoch  1 no_batch  0 Test loss  tensor(1.5342, grad_fn=<MseLossBackward0>)\n",
      "epoch  1 epoch_loss  0.017046484682295058\n",
      "epoch  2 no_batch  0 Train loss  tensor(1.6959, grad_fn=<MseLossBackward0>)\n",
      "epoch  2 no_batch  50 Train loss  tensor(1.6408, grad_fn=<MseLossBackward0>)\n",
      "epoch  2 no_batch  100 Train loss  tensor(2.1361, grad_fn=<MseLossBackward0>)\n",
      "epoch  2 no_batch  150 Train loss  tensor(1.5446, grad_fn=<MseLossBackward0>)\n",
      "epoch  2 no_batch  200 Train loss  tensor(1.6300, grad_fn=<MseLossBackward0>)\n",
      "epoch  2 no_batch  250 Train loss  tensor(1.3798, grad_fn=<MseLossBackward0>)\n",
      "epoch  2 epoch_loss  1.582734574874242\n",
      "epoch  2 no_batch  0 Test loss  tensor(1.5342, grad_fn=<MseLossBackward0>)\n",
      "epoch  2 epoch_loss  0.017046484682295058\n",
      "epoch  3 no_batch  0 Train loss  tensor(1.6959, grad_fn=<MseLossBackward0>)\n",
      "epoch  3 no_batch  50 Train loss  tensor(1.6408, grad_fn=<MseLossBackward0>)\n",
      "epoch  3 no_batch  100 Train loss  tensor(2.1361, grad_fn=<MseLossBackward0>)\n",
      "epoch  3 no_batch  150 Train loss  tensor(1.5446, grad_fn=<MseLossBackward0>)\n",
      "epoch  3 no_batch  200 Train loss  tensor(1.6300, grad_fn=<MseLossBackward0>)\n",
      "epoch  3 no_batch  250 Train loss  tensor(1.3798, grad_fn=<MseLossBackward0>)\n",
      "epoch  3 epoch_loss  1.582734574874242\n",
      "epoch  3 no_batch  0 Test loss  tensor(1.5342, grad_fn=<MseLossBackward0>)\n",
      "epoch  3 epoch_loss  0.017046484682295058\n",
      "epoch  4 no_batch  0 Train loss  tensor(1.6959, grad_fn=<MseLossBackward0>)\n",
      "epoch  4 no_batch  50 Train loss  tensor(1.6408, grad_fn=<MseLossBackward0>)\n",
      "epoch  4 no_batch  100 Train loss  tensor(2.1361, grad_fn=<MseLossBackward0>)\n",
      "epoch  4 no_batch  150 Train loss  tensor(1.5446, grad_fn=<MseLossBackward0>)\n",
      "epoch  4 no_batch  200 Train loss  tensor(1.6300, grad_fn=<MseLossBackward0>)\n",
      "epoch  4 no_batch  250 Train loss  tensor(1.3798, grad_fn=<MseLossBackward0>)\n",
      "epoch  4 epoch_loss  1.582734574874242\n",
      "epoch  4 no_batch  0 Test loss  tensor(1.5342, grad_fn=<MseLossBackward0>)\n",
      "epoch  4 epoch_loss  0.017046484682295058\n"
     ]
    }
   ],
   "source": [
    "batch_size = 50\n",
    "lr = 0.0001\n",
    "epochs = 5\n",
    "\n",
    "data = np.load(\"feature_data.npy\")\n",
    "training_data, testing_data = importdata(data)\n",
    "\n",
    "cnn = ConvNet()\n",
    "optimizer = torch.optim.Adam(cnn.parameters(), lr = lr)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "train_loss_list = []\n",
    "test_loss_list = []\n",
    "epoch_list = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss  = train(training_data, optimizer,criterion,epoch,cnn,scope = 'cl')\n",
    "    test_loss,output_data = test(testing_data,criterion,epoch,epochs,cnn,scope = 'cl')\n",
    "    \n",
    "    train_loss_list.append(train_loss)\n",
    "    test_loss_list.append(test_loss)\n",
    "    \n",
    "    epoch_list.append(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8111abea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 50, 7)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f986217",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data=np.array(output_data)\n",
    "np.save(\"predictions\",output_data)\n",
    "output_data=output_data.reshape((50,7))\n",
    "output_data=output_data.astype(str)\n",
    "np.savetxt(\"NACA_predictions.csv\", output_data, delimiter=\",\", fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d77196ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"train loss \",train_loss_list)\n",
    "np.save(\"test loss \",test_loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2b153004",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.017046484682295058"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d8fcf370",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (100,) and (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [49]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m8\u001b[39m,\u001b[38;5;241m5\u001b[39m))\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain and Test Loss vs Epochs\u001b[39m\u001b[38;5;124m\"\u001b[39m,fontsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx-large\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(epochs,test_loss,label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpochs\u001b[39m\u001b[38;5;124m\"\u001b[39m,fontsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx-large\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\matplotlib\\pyplot.py:2757\u001b[0m, in \u001b[0;36mplot\u001b[1;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2755\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mplot)\n\u001b[0;32m   2756\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot\u001b[39m(\u001b[38;5;241m*\u001b[39margs, scalex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, scaley\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m-> 2757\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m gca()\u001b[38;5;241m.\u001b[39mplot(\n\u001b[0;32m   2758\u001b[0m         \u001b[38;5;241m*\u001b[39margs, scalex\u001b[38;5;241m=\u001b[39mscalex, scaley\u001b[38;5;241m=\u001b[39mscaley,\n\u001b[0;32m   2759\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m: data} \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\matplotlib\\axes\\_axes.py:1632\u001b[0m, in \u001b[0;36mAxes.plot\u001b[1;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1390\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1391\u001b[0m \u001b[38;5;124;03mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[0;32m   1392\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1629\u001b[0m \u001b[38;5;124;03m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[0;32m   1630\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1631\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m cbook\u001b[38;5;241m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[38;5;241m.\u001b[39mLine2D)\n\u001b[1;32m-> 1632\u001b[0m lines \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_lines(\u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39mdata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)]\n\u001b[0;32m   1633\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[0;32m   1634\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_line(line)\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\matplotlib\\axes\\_base.py:312\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[1;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m    310\u001b[0m     this \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    311\u001b[0m     args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m--> 312\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_plot_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\matplotlib\\axes\\_base.py:498\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[1;34m(self, tup, kwargs, return_kwargs)\u001b[0m\n\u001b[0;32m    495\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes\u001b[38;5;241m.\u001b[39myaxis\u001b[38;5;241m.\u001b[39mupdate_units(y)\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[1;32m--> 498\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y must have same first dimension, but \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    499\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhave shapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    500\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m    501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y can be no greater than 2D, but have \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    502\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (100,) and (1,)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs= np.linspace(1,100,100)\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.title(\"Train and Test Loss vs Epochs\",fontsize='x-large')\n",
    "plt.plot(epochs,train_loss,label='Train')\n",
    "plt.plot(epochs,test_loss,label='Test')\n",
    "plt.xlabel(\"Epochs\",fontsize='x-large')\n",
    "plt.ylabel(\"Loss\",fontsize='x-large')\n",
    "plt.legend(fontsize='x-large')\n",
    "plt.savefig('./CNN_loss_plot.png',dpi=288)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a281178a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
